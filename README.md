# üöÄ stack-modern ‚Äî Modern Data Stack (README principal)

**Airbyte ‚Ä¢ Airflow ‚Ä¢ dbt ‚Ä¢ PostgreSQL**

Este reposit√≥rio demonstra uma **plataforma anal√≠tica end-to-end** (production-style) que mostra como times de dados modernos ingere, transforma e serve dados usando uma Modern Data Stack.  
Inclui instru√ß√µes para instalar localmente (Airbyte via `abctl`), orquestrar com Airflow e usar Postgres como data warehouse.

---

## üìÅ Estrutura final do reposit√≥rio

stack-modern/
‚îú‚îÄ‚îÄ README.md ‚Üê P√°gina principal (este arquivo)
‚îú‚îÄ‚îÄ ARCHITECTURE.md ‚Üê Arquitetura t√©cnica (opcional: separado)
‚îú‚îÄ‚îÄ SETUP.md ‚Üê Como rodar tudo local (opcional: separado)
‚îú‚îÄ‚îÄ PIPELINES.md ‚Üê Como os dados fluem (opcional: separado)
‚îú‚îÄ‚îÄ DBT.md ‚Üê Como voc√™ modela dados (opcional: separado)
‚îú‚îÄ‚îÄ AIRFLOW.md ‚Üê Como voc√™ orquestra (opcional: separado)
‚îú‚îÄ‚îÄ DATA.md ‚Üê Fontes de dados (opcional: separado)
‚îî‚îÄ‚îÄ .gitignore

markdown
Copiar c√≥digo

> **Nota r√°pida para recrutadores t√©cnicos:** os quatro arquivos que voc√™ provavelmente abrir√° primeiro s√£o:
> - `README.md` (este)  
> - `SETUP.md`  
> - `ARCHITECTURE.md`  
> - `DBT.md`  

---

## üß† ARCHITECTURE ‚Äî Modern Data Stack

Este projeto implementa uma **Modern Data Stack** com:

- **Airbyte** para ingest√£o
- **PostgreSQL** como Data Warehouse
- **dbt** para transforma√ß√µes
- **Airflow** para orquestra√ß√£o

### High-level flow

Source Systems ‚Üí Airbyte ‚Üí PostgreSQL (raw)
                          ‚Üì
                         dbt
                          ‚Üì
                 PostgreSQL (analytics)
                          ‚Üì
                     BI / SQL
Layers
Layer	Purpose
raw	Dados exatamente como ingeridos pelo Airbyte (raw tables)
staging	Dados limpos e padronizados (stg_*)
marts	Tabelas anal√≠ticas: facts & dimensions otimizadas para BI

Por que essa arquitetura
Este design alinha com pr√°ticas de times de dados reais para garantir:

Qualidade de dados

Escalabilidade

Reprodutibilidade

Modelos prontos para an√°lise

üß∞ SETUP ‚Äî Como rodar local
Requisitos
Docker

Docker Compose (v2 prefer√≠vel)

Git

Python 3.9+ (para dbt local / utilit√°rios)

Pelo menos 8GB de RAM (ideal 12GB+)

1) Iniciar PostgreSQL & Airflow (Docker Compose)
Crie um arquivo docker-compose.yml no reposit√≥rio com o conte√∫do abaixo (exemplo m√≠nimo):

yaml
Copiar c√≥digo
version: "3.8"

services:
  postgres:
    image: postgres:15
    container_name: warehouse
    environment:
      POSTGRES_USER: analytics
      POSTGRES_PASSWORD: analytics
      POSTGRES_DB: analytics
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data

  airflow:
    image: apache/airflow:2.8.1
    container_name: airflow
    environment:
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__EXECUTOR: "SequentialExecutor"
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://analytics:analytics@postgres/analytics"
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username admin --password admin --firstname Guilherme --lastname Stefano --role Admin --email admin@example.com &&
      airflow webserver & airflow scheduler
      "

volumes:
  pgdata:
Suba os servi√ßos:

bash
Copiar c√≥digo
docker compose up -d
Acesse:

Airflow UI: http://localhost:8080 (user: admin / pass: admin)

Postgres: host=localhost, port=5432, user=analytics, password=analytics, db=analytics

Observa√ß√£o: para ambientes Docker em Mac/Windows, se precisar que containers acessem servi√ßos host, use host.docker.internal como host para conex√µes a servi√ßos rodando na m√°quina host.

2) Instalar e rodar Airbyte via abctl (local)
abctl √© o instalador CLI oficial do Airbyte para setups locais.

Instalar abctl:

bash
Copiar c√≥digo
curl -LsfS https://get.airbyte.com | bash
Verifique a vers√£o:

bash
Copiar c√≥digo
abctl version
Instalar Airbyte localmente:

bash
Copiar c√≥digo
abctl local install
Isso:

cria um cluster/local runtime e instala Airbyte (k8s/kit usado pelo abctl)

exp√µe UI do Airbyte em http://localhost:8000

Abra http://localhost:8000 e siga o assistente para criar sources e destinations.

3) Conectar Airbyte ‚Üí PostgreSQL
No Airbyte UI:

Create Destination

Tipo: PostgreSQL

Host: host.docker.internal ou postgres (se voc√™ rodar tudo no mesmo compose e apontar via network)

Port: 5432

Database: analytics

User: analytics

Password: analytics

Schema: raw

Create Source (ex.: API p√∫blica, MySQL local, CSV, etc.)

Create Connection

Sync frequency: conforme desejar (manual, hourly, daily)

Namespace / Schema: raw

Modo: incremental quando dispon√≠vel (CDC) ou full-refresh conforme o caso

Depois do sync, dados aparecer√£o como analytics.raw.<nome_da_tabela>.

4) Instalar e configurar dbt (local)
Instale o adaptador Postgres do dbt:

bash
Copiar c√≥digo
pip install dbt-postgres
Inicie um projeto dbt:

bash
Copiar c√≥digo
dbt init analytics_platform
cd analytics_platform
Exemplo m√≠nimo de profiles.yml (em ~/.dbt/profiles.yml):

yaml
Copiar c√≥digo
analytics_platform:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      user: analytics
      password: analytics
      port: 5432
      dbname: analytics
      schema: analytics
Verifique a conex√£o:

bash
Copiar c√≥digo
dbt debug
Rodar modelos:

bash
Copiar c√≥digo
dbt run
dbt test
dbt docs generate
dbt docs serve
üîÅ PIPELINES ‚Äî Fluxo dos dados
Ingest√£o
Airbyte extrai dados de APIs / DBs e grava no schema raw do Postgres:
analytics.raw.*

Transforma√ß√£o
dbt cria camadas:

staging (stg_*) ‚Äî limpeza e padroniza√ß√£o

marts ‚Äî facts & dims prontos para BI

Orquestra√ß√£o
Airflow DAG (exemplo) executa em sequ√™ncia:

Trigger Airbyte sync (via API)

dbt run

dbt test

Notifica√ß√£o / valida√ß√£o

Fluxo l√≥gico:

text
Copiar c√≥digo
Airbyte Sync ‚Üí dbt run ‚Üí dbt test ‚Üí (alerts)
üß± DBT ‚Äî Como modelar
Estrutura sugerida
pgsql
Copiar c√≥digo
models/
  staging/
    stg_customers.sql
    stg_orders.sql
  marts/
    dim_customers.sql
    fact_orders.sql
Boas pr√°ticas
Use ref() e evite hard-coded table names.

Separe camadas: staging ‚Üí marts.

Escreva testes (not_null, unique, relationships).

Documente modelos com schema.yml.

Use incremental models quando a fonte permitir.

Exemplo simples de fact_orders.sql:

sql
Copiar c√≥digo
select
  order_id,
  customer_id,
  order_date,
  total_amount
from {{ ref('stg_orders') }}
Comandos dbt comuns:

bash
Copiar c√≥digo
dbt run --models marts
dbt test --models +marts
dbt docs generate
‚è± AIRFLOW ‚Äî Orquestra√ß√£o
Airflow orquestra a execu√ß√£o dos passos do pipeline. Um DAG t√≠pico deve:

Fazer chamada √† API do Airbyte para iniciar o sync (Airbyte API)

Aguardar conclus√£o / checar status

Executar dbt run (via BashOperator ou DockerOperator)

Executar dbt test

Emitir alertas (Slack / email) em caso de falha

DAG flow (visual):

text
Copiar c√≥digo
airbyte_sync_task -> dbt_run_task -> dbt_test_task -> notify_task
Observa√ß√£o: para integra√ß√£o Airbyte ‚Üî Airflow, existem patterns:

Usar o requests para chamar a API do Airbyte (start sync / check job status)

Usar DockerOperator ou KubernetesPodOperator para rodar dbt de forma isolada

üìä DATA ‚Äî Fontes de dados
Este projeto suporta:

REST APIs (ex.: JSON p√∫blicos)

Bancos relacionais (MySQL/Postgres)

CSVs / arquivos locais (upload via Airbyte)

Event streams (quando usar Kafka)

Fluxo: toda fonte ‚Üí raw ‚Üí staging ‚Üí marts ‚Üí analytics
