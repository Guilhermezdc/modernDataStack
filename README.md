# ğŸš€ stack-modern â€” Modern Data Stack (README principal)

**Airbyte â€¢ Airflow â€¢ dbt â€¢ PostgreSQL**

Este repositÃ³rio demonstra uma **plataforma analÃ­tica end-to-end** (production-style) que mostra como times de dados modernos ingere, transforma e serve dados usando uma Modern Data Stack.  
Inclui instruÃ§Ãµes para instalar localmente (Airbyte via `abctl`), orquestrar com Airflow e usar Postgres como data warehouse.

---

## ğŸ“ Estrutura final do repositÃ³rio

```md
## ğŸ“‚ Pastas principais

- ğŸ“ [`config/`](config) â€” ConfiguraÃ§Ãµes do Airflow  
- ğŸ“ [`dags/`](dags) â€” DAGs  
- ğŸ“ [`prodDataBuilder/`](prodDataBuilder) â€” dbt (models, macros, analyses)  
- ğŸ“„ [`Dockerfile`](Dockerfile)

```
---

## ğŸ§  ARCHITECTURE â€” Modern Data Stack

Este projeto implementa uma **Modern Data Stack** com:

- **Airbyte** para ingestÃ£o
- **PostgreSQL** como Data Warehouse
- **dbt** para transformaÃ§Ãµes
- **Airflow** para orquestraÃ§Ã£o

### ğŸ”„ High-level flow

```text
Source Systems
      â”‚
      â–¼
   Airbyte
      â”‚
      â–¼
PostgreSQL (raw)
      â”‚
      â–¼
     dbt
      â”‚
      â–¼
PostgreSQL (analytics)
      â”‚
      â–¼
   BI / SQL
```

Layers

Layer	Purpose
raw	Dados exatamente como ingeridos pelo Airbyte (raw tables)
staging	Dados limpos e padronizados (stg_*)
marts	Tabelas analÃ­ticas: facts & dimensions otimizadas para BI

Por que essa arquitetura
Este design alinha com prÃ¡ticas de times de dados reais para garantir:

Qualidade de dados

Escalabilidade

Reprodutibilidade

Modelos prontos para anÃ¡lise

## ğŸ§° SETUP â€” InstalaÃ§Ã£o Local

Requisitos
Docker & Docker Compose (V2 preferÃ­vel)

Git & Python 3.9+

MemÃ³ria: MÃ­nimo 8GB RAM (12GB+ recomendado)

### 1) Iniciar PostgreSQL (Docker Compose)

Antes de tudo vocÃª precisar de criar um arquivo .envS

Suba o banco de dados:

```bash
docker-compose -f docker-compose-postgres.yaml up -d
```
Depois suba os serviÃ§os do airflow:

```bash
docker compose up -d
```
Acesse:

Airflow UI: http://localhost:8080 (user: admin / pass: admin)

Postgres: host=localhost, port=5432, user=root, password=`2skj(Hk2hksf2`, db=analytics

ObservaÃ§Ã£o: para ambientes Docker em Mac/Windows, se precisar que containers acessem serviÃ§os host, use host.docker.internal como host para conexÃµes a serviÃ§os rodando na mÃ¡quina host.

### 2) Instalar e rodar Airbyte via abctl (local)

abctl Ã© o instalador CLI oficial do Airbyte para setups locais.

Instalar abctl:

```bash
curl -LsfS https://get.airbyte.com | bash
```
Instalar Airbyte localmente:

```bash
abctl local install
```
Isso:

cria um cluster/local runtime e instala Airbyte (k8s/kit usado pelo abctl)

expÃµe UI do Airbyte em http://localhost:8000

Abra http://localhost:8000 e siga o assistente para criar sources e destinations.

### 3) Conectar Airbyte â†’ PostgreSQL
No Airbyte UI:

Create Destination

Tipo: PostgreSQL

Host: host.docker.internal ou postgres (se vocÃª rodar tudo no mesmo compose e apontar via network)

Port: 5432

Database: analytics

User: root

Password: `2skj(Hk2hksf2`

Schema: raw

Create Source (ex.: API pÃºblica, MySQL local, CSV, etc.)

Create Connection

Sync frequency: conforme desejar (manual, hourly, daily)

Namespace / Schema: raw

Modo: incremental quando disponÃ­vel (CDC) ou full-refresh conforme o caso

Depois do sync, dados aparecerÃ£o como analytics.raw.<nome_da_tabela>.

### 4) Instalar e configurar dbt (local)

> Notas: Nesse projeto ao rodar o docker-compose o dbt jÃ¡ Ã© configurado automaticamente, porÃ©m, vou deixar uma breve explicaÃ§Ã£o.
Instale o adaptador Postgres do dbt:

```bash
Copiar cÃ³digo
pip install dbt-core
pip install dbt-postgres
```
Inicie um projeto dbt:

```bash
dbt init prodDataBuilder
cd analytics_platform
```
Exemplo mÃ­nimo de profiles.yml (em ~/.dbt/profiles.yml):

```yaml
analytics_platform:
  target: dev
  outputs:
    dev:
      type: postgres
      host: localhost
      user: root
      password: 2skj(Hk2hksf2
      port: 5432
      dbname: analytics
      schema: analytics
```
Verifique a conexÃ£o:

bash
Copiar cÃ³digo
dbt debug
Rodar modelos:

bash
Copiar cÃ³digo
dbt run
dbt test
dbt docs generate
dbt docs serve

## ğŸ” PIPELINES â€” Fluxo dos dados
IngestÃ£o
Airbyte extrai dados de APIs / DBs e grava no schema raw do Postgres:
analytics.raw.*

TransformaÃ§Ã£o
dbt cria camadas:

staging (stg_*) â€” limpeza e padronizaÃ§Ã£o

marts â€” facts & dims prontos para BI

OrquestraÃ§Ã£o
Airflow DAG (exemplo) executa em sequÃªncia:

Trigger Airbyte sync (via API)

dbt run

dbt test

NotificaÃ§Ã£o / validaÃ§Ã£o

Fluxo lÃ³gico:

text
Copiar cÃ³digo
Airbyte Sync â†’ dbt run â†’ dbt test â†’ (alerts)

## ğŸ§± DBT â€” Como modelar
Estrutura sugerida
pgsql
Copiar cÃ³digo
models/
  staging/
    stg_customers.sql
    stg_orders.sql
  marts/
    dim_customers.sql
    fact_orders.sql
Boas prÃ¡ticas
Use ref() e evite hard-coded table names.

Separe camadas: staging â†’ marts.

Escreva testes (not_null, unique, relationships).

Documente modelos com schema.yml.

Use incremental models quando a fonte permitir.

Exemplo simples de fact_orders.sql:

sql
Copiar cÃ³digo
select
  order_id,
  customer_id,
  order_date,
  total_amount
from {{ ref('stg_orders') }}
Comandos dbt comuns:

bash
Copiar cÃ³digo
dbt run --models marts
dbt test --models +marts
dbt docs generate
â± AIRFLOW â€” OrquestraÃ§Ã£o
Airflow orquestra a execuÃ§Ã£o dos passos do pipeline. Um DAG tÃ­pico deve:

Fazer chamada Ã  API do Airbyte para iniciar o sync (Airbyte API)

Aguardar conclusÃ£o / checar status

Executar dbt run (via BashOperator ou DockerOperator)

Executar dbt test

Emitir alertas (Slack / email) em caso de falha

DAG flow (visual):

text
Copiar cÃ³digo
airbyte_sync_task -> dbt_run_task -> dbt_test_task -> notify_task
ObservaÃ§Ã£o: para integraÃ§Ã£o Airbyte â†” Airflow, existem patterns:

Usar o requests para chamar a API do Airbyte (start sync / check job status)

Usar DockerOperator ou KubernetesPodOperator para rodar dbt de forma isolada

ğŸ“Š DATA â€” Fontes de dados
Este projeto suporta:

REST APIs (ex.: JSON pÃºblicos)

Bancos relacionais (MySQL/Postgres)

CSVs / arquivos locais (upload via Airbyte)

Event streams (quando usar Kafka)

Fluxo: toda fonte â†’ raw â†’ staging â†’ marts â†’ analytics
